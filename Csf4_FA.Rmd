---
output: html_document
chapter: Factor Analysis and CSF 
editor_options:
  chunk_output_type: console
---

# Factor Analysis and CSF (in development)

```{r, message=F, warning=F}
library(tidyverse)
library(cowplot)
library(smplot2)
library(smCSF)
```

In this chapter, we will use a public dataset of achromatic contrast sensitivity from 51 normally sighted observers.

```{r, message=F, warning=F}
df <- read_csv('https://www.smin95.com/data_ACh.csv') %>%
  group_by(Subject, SpatialFreq) %>% 
  summarise(Sensitivity = mean(Sensitivity),
            Repetition = 'avg')

```

## Factor Analysis

Factor analysis is a technique to identify latent factors that produce local patterns/correlations in a high-dimensional dataset. It aims to reduce the number of dimensions in the data.

One way to extract an adequate number of factors/dimensions is to draw a scree plot. 

## Eigenvalues

The scree plot has a y-axis of eigenvalues, which means the amount of variance of the data from the factor of interest. Essentially, the higher the eigenvalue, the more important the factor is in the dataset.

Some methods for factor analysis determine the number of factors in a potential factor model based on eigenvalues. The Guttman Rule extracts all factors with eigenvalues larger than 1.

However, this is problematic because in one dataset, factor A can have an eigenvalue larger than 1 but in another dataset from another population factor B can have an eigenvalue lower than 1. Therefore the number of factors that is extracted can vary depending on the local sample. This creates variability in results. Therefore, it might be better to include a range of uncertainty of eigenvalues so that one can better be informed about the possible range of eigenvalues in a population. This can be done by plotting 95% confidence interval of eigenvalues. 

In the method below, we will perform non-parametric bootstrapping to compute the confidence intervals of the eigenvalues. However, it turns out, if we resample the contrast sensitivity data directly, it removes the local correlation and therefore the influence of latent factors on the dataset, thereby erasing covariance from its existence. Therefore, a solution is to use a model that captures the contrast sensitivity function, compute parameters that retain the overall characteristic of the curve for each subject, and then resample the parameters by resampling. In this example, we will resample for 100 times (`nSim <- 100`).

Calculation of the parameters from a data frame that contains contrast sensitivity data can be perfomed with `sm_params_list()`.

```{r}
param_res <- sm_params_list(subjects = 'Subject', 
                                   conditions = 'Repetition',
                                   x = 'SpatialFreq', 
                                   values = 'Sensitivity', 
                                   data = df)

```

After we compute the four parameters of the parabola model of CSF for each subject, we resample them with replacement. Note that the sample size with resampled data should be identical to the original sample size (`nObs <- 51`).


```{r}
nObs <- 51
nSim <- 100 # 100 simulations
```

Then, we can perform non-parametric bootstrapping and obtain 95% confidence interval of the eigenvalues using `sm_np_boot()`. It produces a data frame with the confidence intervals from resampling and observed eigenvalue from the original parameters. This is stored on `boot_res`.

```{r,  eval = F}
boot_res <- sm_np_boot(param_res, n=nObs, nSim = nSim)
boot_res
```

```{r, echo = F, message=F, warning=F}
boot_res <- read_csv('boot_res.csv')
boot_res
```

By using `sm_plot_boot()`, we can then draw the scree plot with the eigenvalues and their 95% confidence intervals.

```{r, fig.width = 3.8, fig.height=3.2}
sm_plot_boot(boot_res, shapes = 16) + 
  ylab('Eigenvalue') -> eigen1

eigen1
```

We can then extract the number of factors where the 95% confidence intervals of the eigenvalues from the observed data do not overlap with those from randomly generated data from parallel analysis. In this example, we can develop an exploratory model with two factors.
